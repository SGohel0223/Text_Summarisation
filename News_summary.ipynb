{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3baf32b9-56a4-4ce9-b0f1-ccc9e379e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_data():\n",
    "    dataset = load_dataset(\"multi_news\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6545172-8ec9-45ef-8114-6f214683dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe77a25-c58e-4451-bee6-dca05ac40871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a89f7d-b46a-41bb-9c36-6d6bc29a2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_nm = 't5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)\n",
    "\n",
    "def tokenize_data(x):\n",
    "  model_inputs = tokenizer(\n",
    "      x['document'],\n",
    "      max_length = 512,\n",
    "      padding=True,\n",
    "      truncation=True\n",
    "  )\n",
    "  labels = tokenizer(\n",
    "      x['summary'],\n",
    "      max_length = 512,\n",
    "      padding = True,\n",
    "      truncation=True\n",
    "  )\n",
    "  model_inputs['labels'] = labels['input_ids']\n",
    "  return model_inputs\n",
    "\n",
    "def preprocess():\n",
    "    dataset = get_data()\n",
    "    tok_ds = dataset.map(tokenize_data, batched=True)\n",
    "    return tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e17f0f-eb64-48b1-960a-7f55084d0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration\n",
    "\n",
    "def train_model(tok_ds,num_train_epochs,batch_size):\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_ds[\"train\"],\n",
    "    eval_dataset=tok_ds[\"validation\"],\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_rouge_scores(\n",
    "        tokenizer.batch_decode(p.predictions, skip_special_tokens=True),\n",
    "        tokenizer.batch_decode(p.label_ids, skip_special_tokens=True),\n",
    "        ),\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e133aa-b454-4f72-a3d0-9cba5445c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer):\n",
    "    eval_metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2306f1d-0526-4398-8e2f-3cb7f2bc4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_model(trainer):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "    text = input(\"Enter the text you want to summarize: \")\n",
    "    tokenized = tokenize_for_inference(text)\n",
    "    generated = trainer.model.generate(tokenized, max_length=256)\n",
    "    \n",
    "    # Convert the generated output back to text\n",
    "    summary = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "    print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d1b680-26fa-4451-a38f-c223664f520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(num_train_epochs,batch_size):\n",
    "    tok_ds = preprocess()\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer,model=model,return_tensors='pt')\n",
    "    trainer = train_model(tok_ds, num_train_epochs, batch_size)\n",
    "    trained_model = trainer.model\n",
    "    eval_metric = evaluate_model(trainer)\n",
    "    infer_model(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579151a3-1223-4529-88c0-c2a57f202110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb1377-71c4-4683-8263-723bbe47d9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faef94-565c-40d4-ada1-ee73d887dc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
